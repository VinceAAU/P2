\subsection{Task Scheduling}
In a grid computing system, tasks must be scheduled to the worker nodes appropriately so as to not waste the resources offered by the grid. Grids use a task scheduler to achieve this. The task scheduler is responsible for receiving, keeping track of, and delegating jobs to the worker nodes, ideally, finding the most suitable node for the job in question. There are several ways to structure a task scheduler, each with its own advantages and disadvantages \cite{TaskSchedulingReview}.

\subsubsection{Centralized Scheduling}
In a centralized system, the tasks are distributed to the worker nodes by a server. This is a relatively simple way of creating a task scheduler, but it has some weaknesses. For large systems with many job requests and nodes, the master node itself can require significant computational resources for the task of scheduling, which can cause a bottleneck and ultimately lead to inefficiency. A centralized system is also prone to system failure, as the entire system will cease working if the server fails. This type of system scales vertically, as the central unit will have to be upgraded to handle more nodes. Centralized systems are mostly used for their cost-effectiveness in relatively small grids and their ease of use. Having a centralized unit also makes security issues simpler to solve. \cite{TaskSchedulingReview, SchedulingInGridComputing}.

\subsubsection{Distributed Scheduling}
In distributed scheduling, there is no central unit issuing orders to the worker nodes in the system. This can be done by allowing each node to communicate with all other nodes in the grid for information about their availability and workload and using a local scheduler to determine the appropriate task to start. This type of scheduling prevents system-wide failure from a single node failing. If a node fails, another node will eventually complete the job. This, however, does not prevent a distributed scheduling system from producing a faulty result, as it can be difficult to implement proper validation of completed jobs. This system also requires more communication between nodes than centralized scheduling, which can cause a bottleneck as the number of nodes increases \cite{SchedulingInGridComputing, Systems_geeks}.

\subsubsection{Hierarchical Scheduling}
As opposed to distributed and centralized scheduling, hierarchical scheduling uses more than two types of nodes. This system works by dividing the grid into several sub-grids using a main scheduler node and handing over command to several manager nodes. The hierarchy can then be expanded with more intermediate manager nodes for more localization. Depending on implementation, this type of scheduler can still be prone to system-wide failure, if the main scheduler node fails. However, the system will not fail completely, as the rest of the grid can handle any jobs in queue, but they may not be able to accept new jobs. As the main scheduler has a minimal workload, and all other nodes' workload can be lessened by simply adding more nodes, this scheduler scales well horizontally \cite{Behnam1330, TaskSchedulingReview}.

\subsubsection{Resource-based Scheduling}
Nodes will often have vastly different capabilities in terms of processing power, especially in volunteer grid computing systems. This can lead to inefficiency if not taken into account during task scheduling. If each node is assigned a similar task, the slower nodes will slow down the whole system, while the fast nodes idle. There are several ways to account for this depending on the type of scheduler used. The biggest issue with optimizing for worker resources is the communication overhead. Taking the capabilities of each node into account often requires frequent communication between the scheduler and worker nodes, as the processing power is subject to change. If the owner of a worker node in a volunteer grid computing system opens another resource intensive task, it may fall behind on its task assigned by the scheduler, leading to it slowing down the system. To avoid this, communication between the worker nodes and the scheduler(s) must be maintained actively, and not just at the start of a task \cite{abhang_2012}.
\chapter{Discussion and Future Developments}
This chapter will include a discussion of the project and its results, as well as what can be done to further improve the product, if this project was to continue beyond the current limitations.

\section{Discussion}
During this project a functional grid computing system was developed. This system makes it possible to upload a CSV file and have it divided into pieces that can be sorted on several different devices simultaneously. The product also includes a simple web site, which creates a pleasant user experience, since the functionality is in focus.
 
However, while the sorting section of the product lives up to expectations, there are sadly several factors detrimental to the usefulness of the product. These are mainly communication overhead, when communicating with the worker nodes, and loading and writing the file as mentioned in Section \ref{sssec:timeEfficient}. How the communication overhead and the time it takes to write the sorted file might be mitigated will be discussed in Section \ref{sec:futuredev}. 

While these issues affect the time efficiency of the product, other areas should also be taken into consideration, mainly available disk space and random-access memory (RAM) capacity. Unfortunately, the server used to test the program was not optimal as it lacked both the hard disk and RAM capacity to properly test the benefits of the system. This means that it has not been possible to show how the grid computing system can outperform a single computer in terms of RAM scaling. However, it was enough to have a proof of concept when considering how the grid computing system affects the time that it takes to sort a file. 

When taking into account that this product is aimed towards big data, the loading of the data is handled poorly. This is because the entire file is loaded into the RAM, every time a new task is started. This means that if the task is larger than the available RAM it would start using the slower swap memory to handle the file. It also meant that extra attention had to be paid during the writing of the program to limit anything that might take up already limited RAM. A better way to handle loading the file into memory would be to load it in segments that fits into the RAM. This would enable the program to handle much larger files, in theory up to half the available disk space, as it would also need to save the file both while working and when it is done sorting. 

This method would make it possible to get workers waiting for tasks to work earlier than the current system, as it would not need to load an entire file into memory before allocating tasks. It might, however, be beneficial to still load in chunks of data that are 2-4 GB or bigger in size, as this should result in a better spread of values. There are, however, also downsides to this approach, as it would create a much larger workload. Not because of a larger file, but simply because you would have to sort the buckets twice. Once when you first load in your chunk of data and once more when placing the data in regards to other buckets within the same range of data values. Furthermore you would have more writing and reading operations to save and reload the sorted buckets. This makes this method bad for the proof of concept, since it introduces multiple factors that negatively affect the result. These factors are not relevant for the file sizes used in the proof of concept, since they can be handled in RAM alone. There is also the factor that any file, where this method is beneficial, would have the same negative factors for a single computer, as it would not be able to handle the file otherwise. 

This means that the current way of handling the files is the best for the proof of concept, but that it should be substituted for the more complex method that can handle larger files, if filling its actual function of handling big data.

\input{sections/6.Discussion/futureDevelopment}
